#!/usr/bin/env python3
"""
Odoo Turkey Partners Web Scraper
================================
Bu script Odoo'nun TÃ¼rkiye partner sayfasÄ±ndan partner bilgilerini Ã§eker.

KullanÄ±m:
    python odoo_partner_scraper.py

Gereksinimler:
    pip install requests beautifulsoup4 pandas

Ã‡Ä±ktÄ±:
    - odoo_partners.csv: TÃ¼m partner bilgilerini iÃ§eren CSV dosyasÄ±
    - odoo_partners.json: TÃ¼m partner bilgilerini iÃ§eren JSON dosyasÄ±
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import re
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
from datetime import datetime


@dataclass
class Partner:
    """Partner veri modeli"""
    name: str
    level: str  # Gold, Silver, Ready
    city: str
    district: str
    country: str
    full_address: str
    rating_percentage: Optional[str]
    average_project_size: Optional[str]
    large_project_size: Optional[str]
    references_count: str
    certified_experts_count: str
    profile_url: str
    logo_url: Optional[str]
    scraped_at: str


class OdooPartnerScraper:
    """Odoo partner bilgilerini Ã§eken scraper sÄ±nÄ±fÄ±"""
    
    BASE_URL = "https://www.odoo.com"
    PARTNERS_URL = "https://www.odoo.com/tr_TR/partners/country/turkiye-215"
    
    def __init__(self, delay_seconds: float = 1.0):
        """
        Args:
            delay_seconds: Her istek arasÄ±nda beklenecek sÃ¼re (saniye)
        """
        self.delay_seconds = delay_seconds
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
        })
    
    def _get_page(self, url: str) -> Optional[BeautifulSoup]:
        """URL'den sayfa iÃ§eriÄŸini Ã§eker"""
        try:
            print(f"ğŸ“¥ Sayfa Ã§ekiliyor: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except requests.RequestException as e:
            print(f"âŒ Hata: {url} Ã§ekilemedi - {e}")
            return None
    
    def _extract_text(self, element, default: str = "") -> str:
        """Element'ten text Ã§Ä±karÄ±r"""
        if element:
            return element.get_text(strip=True)
        return default
    
    def _parse_partner_card(self, card) -> Optional[Partner]:
        """Partner kartÄ±ndan bilgileri Ã§Ä±karÄ±r"""
        try:
            # Partner profil URL'i
            profile_url = card.get('href', '')
            if profile_url and not profile_url.startswith('http'):
                profile_url = self.BASE_URL + profile_url
            
            # Partner adÄ± - h5 iÃ§indeki ilk span
            name_element = card.select_one('h5 span:first-child')
            name = self._extract_text(name_element, "Bilinmiyor")
            
            # Partner seviyesi (Gold, Silver, Ready)
            level_element = card.select_one('h5 span.badge')
            level = self._extract_text(level_element, "")
            
            # Logo URL
            logo_element = card.select_one('img')
            logo_url = None
            if logo_element:
                logo_url = logo_element.get('src', '')
                if logo_url and not logo_url.startswith('http'):
                    logo_url = self.BASE_URL + logo_url
            
            # Rating yÃ¼zdesi
            rating_element = card.select_one('span.text-warning')
            rating_percentage = None
            if rating_element:
                rating_text = self._extract_text(rating_element.find_next_sibling())
                if rating_text:
                    rating_percentage = rating_text
            
            # Lokasyon bilgileri - small tag iÃ§indeki spanlar
            location_spans = card.select('small span')
            city = ""
            district = ""
            country = "TÃ¼rkiye"
            full_address = ""
            
            if location_spans:
                location_texts = [self._extract_text(span) for span in location_spans]
                location_texts = [t for t in location_texts if t]  # BoÅŸ olanlarÄ± filtrele
                full_address = ", ".join(location_texts)
                
                # Genellikle: Ä°lÃ§e, Ä°l, Ãœlke formatÄ±nda
                if len(location_texts) >= 3:
                    district = location_texts[0]
                    city = location_texts[1]
                    country = location_texts[2]
                elif len(location_texts) == 2:
                    city = location_texts[0]
                    country = location_texts[1]
                elif len(location_texts) == 1:
                    city = location_texts[0]
            
            # Proje bÃ¼yÃ¼klÃ¼kleri
            all_small = card.select('small')
            average_project_size = None
            large_project_size = None
            
            for small in all_small:
                text = self._extract_text(small)
                if 'Ortalama Proje' in text or 'ortalama' in text.lower():
                    # KullanÄ±cÄ± sayÄ±sÄ±nÄ± Ã§Ä±kar
                    match = re.search(r'(\d+)\s*kullanÄ±cÄ±', text, re.IGNORECASE)
                    if match:
                        average_project_size = f"{match.group(1)} kullanÄ±cÄ±"
                elif 'BÃ¼yÃ¼k Proje' in text or 'bÃ¼yÃ¼k' in text.lower():
                    match = re.search(r'~?(\d+)\s*kullanÄ±cÄ±', text, re.IGNORECASE)
                    if match:
                        large_project_size = f"~{match.group(1)} kullanÄ±cÄ±"
            
            # Referans sayÄ±sÄ±
            references_count = "0"
            for div in card.select('div'):
                text = self._extract_text(div)
                if 'Referans' in text:
                    match = re.search(r'(\d+)\s*Referans', text)
                    if match:
                        references_count = match.group(1)
                    break
            
            # SertifikalÄ± uzman sayÄ±sÄ±
            certified_experts_count = "0"
            for div in card.select('div'):
                text = self._extract_text(div)
                if 'SertifikalÄ± Uzman' in text:
                    match = re.search(r'(\d+)\s*SertifikalÄ±', text)
                    if match:
                        certified_experts_count = match.group(1)
                    break
            
            return Partner(
                name=name,
                level=level,
                city=city,
                district=district,
                country=country,
                full_address=full_address,
                rating_percentage=rating_percentage,
                average_project_size=average_project_size,
                large_project_size=large_project_size,
                references_count=references_count,
                certified_experts_count=certified_experts_count,
                profile_url=profile_url,
                logo_url=logo_url,
                scraped_at=datetime.now().isoformat()
            )
            
        except Exception as e:
            print(f"âš ï¸ Partner parse hatasÄ±: {e}")
            return None
    
    def _get_total_pages(self, soup: BeautifulSoup) -> int:
        """Toplam sayfa sayÄ±sÄ±nÄ± bulur"""
        try:
            # Pagination linklerini bul
            pagination = soup.select('.pagination .page-link')
            max_page = 1
            
            for link in pagination:
                href = link.get('href', '')
                # /page/X formatÄ±nÄ± ara
                match = re.search(r'/page/(\d+)', href)
                if match:
                    page_num = int(match.group(1))
                    max_page = max(max_page, page_num)
            
            return max_page
        except Exception:
            return 1
    
    def scrape_all_partners(self) -> List[Partner]:
        """TÃ¼m partnerleri Ã§eker"""
        partners = []
        
        # Ä°lk sayfayÄ± Ã§ek
        soup = self._get_page(self.PARTNERS_URL)
        if not soup:
            print("âŒ Ana sayfa Ã§ekilemedi!")
            return partners
        
        # Toplam sayfa sayÄ±sÄ±nÄ± bul
        total_pages = self._get_total_pages(soup)
        print(f"ğŸ“Š Toplam {total_pages} sayfa bulundu")
        
        # TÃ¼m sayfalarÄ± dolaÅŸ
        for page_num in range(1, total_pages + 1):
            if page_num == 1:
                page_soup = soup
            else:
                time.sleep(self.delay_seconds)
                page_url = f"{self.PARTNERS_URL}/page/{page_num}"
                page_soup = self._get_page(page_url)
                if not page_soup:
                    continue
            
            # Partner kartlarÄ±nÄ± bul
            # Selector: a.text-decoration-none.row
            partner_cards = page_soup.select('a.text-decoration-none.row.p-2')
            
            if not partner_cards:
                # Alternatif selector dene
                partner_cards = page_soup.select('a[href*="/partners/"]')
                partner_cards = [c for c in partner_cards if 'row' in c.get('class', [])]
            
            print(f"ğŸ“„ Sayfa {page_num}: {len(partner_cards)} partner bulundu")
            
            for card in partner_cards:
                partner = self._parse_partner_card(card)
                if partner and partner.name != "Bilinmiyor":
                    partners.append(partner)
                    print(f"  âœ… {partner.name} ({partner.level})")
        
        return partners
    
    def save_to_csv(self, partners: List[Partner], filename: str = "odoo_partners.csv"):
        """Partnerleri CSV dosyasÄ±na kaydeder"""
        if not partners:
            print("âš ï¸ Kaydedilecek partner yok!")
            return
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=asdict(partners[0]).keys())
            writer.writeheader()
            for partner in partners:
                writer.writerow(asdict(partner))
        
        print(f"ğŸ’¾ {len(partners)} partner {filename} dosyasÄ±na kaydedildi")
    
    def save_to_json(self, partners: List[Partner], filename: str = "odoo_partners.json"):
        """Partnerleri JSON dosyasÄ±na kaydeder"""
        if not partners:
            print("âš ï¸ Kaydedilecek partner yok!")
            return
        
        data = {
            "scraped_at": datetime.now().isoformat(),
            "total_partners": len(partners),
            "source_url": self.PARTNERS_URL,
            "partners": [asdict(p) for p in partners]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ {len(partners)} partner {filename} dosyasÄ±na kaydedildi")


def main():
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
    print("=" * 60)
    print("ğŸš€ Odoo TÃ¼rkiye Partner Scraper")
    print("=" * 60)
    print()
    
    # Scraper'Ä± baÅŸlat
    scraper = OdooPartnerScraper(delay_seconds=1.5)
    
    # TÃ¼m partnerleri Ã§ek
    partners = scraper.scrape_all_partners()
    
    print()
    print("-" * 60)
    print(f"ğŸ“Š Toplam {len(partners)} partner Ã§ekildi")
    print("-" * 60)
    
    if partners:
        # CSV olarak kaydet
        scraper.save_to_csv(partners)
        
        # JSON olarak kaydet
        scraper.save_to_json(partners)
        
        print()
        print("âœ… Ä°ÅŸlem tamamlandÄ±!")
        print()
        
        # Ã–zet gÃ¶ster
        print("ğŸ“ˆ Partner Seviyeleri:")
        levels = {}
        for p in partners:
            level = p.level if p.level else "BelirtilmemiÅŸ"
            levels[level] = levels.get(level, 0) + 1
        for level, count in sorted(levels.items()):
            print(f"   â€¢ {level}: {count}")
        
        print()
        print("ğŸ™ï¸ Åehirlere GÃ¶re DaÄŸÄ±lÄ±m:")
        cities = {}
        for p in partners:
            city = p.city if p.city else "BelirtilmemiÅŸ"
            cities[city] = cities.get(city, 0) + 1
        for city, count in sorted(cities.items(), key=lambda x: -x[1])[:10]:
            print(f"   â€¢ {city}: {count}")
    else:
        print("âŒ HiÃ§ partner Ã§ekilemedi. LÃ¼tfen baÄŸlantÄ±nÄ±zÄ± kontrol edin.")


if __name__ == "__main__":
    main()
